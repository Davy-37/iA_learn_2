# -*- coding: utf-8 -*-
"""Copie de IA-IntroRL-ENSISA-2A-TD1-Prbl2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oazuORt788rwsQRHw5LUYhqSS2GK2wa8

> Import libraries to use
"""

import numpy as np

""">  # Introduction to numpy (Skip if you already are familiar)

>> Creating a 1D array
"""

a = np.array([1,2,3,4])
print(a)

""">> Creating a 2D array

"""

a = np.array([[1,2],[3,4]])
print(a)

""">> Creating an array full of zeros

"""

a = np.zeros(shape=(10))
print(a)
a = np.zeros(shape=(5,2))
print(a)

""">> Infinity in numpy"""

print(np.inf)

""">> Max and Argmax"""

a = np.array([2,1,4,3])
print(np.max(a))
print(np.argmax(a))

""">> From list to Numpy"""

l = [1,2,3,4]
print(l)
print(np.asarray(l))

""">> Random in numpy"""

# Array of Random integers ranging from 1 to 10 (with any size you want)
a = np.random.randint(low=1, high=10, size=(5,2))
print(a)

# Array of random elements of a list with any size you want
a = np.random.choice([0,1,2], size=(2,))

""">> Shapes in numpy"""

a = np.random.randint(low=1, high=5, size=(4,2))
print(a.shape)
print(a)

# Reshape a to a vector of shape = (8,1)
a = a.reshape((8,1))
print(a.shape)
print(a)

"""# Pre-defined utilities"""

int_to_char = {
    0 : 'u',
    1 : 'r',
    2 : 'd',
    3 : 'l'
}

policy_one_step_look_ahead = {
    0 : [-1,0],
    1 : [0,1],
    2 : [1,0],
    3 : [0,-1]
}

def policy_int_to_char(pi,n):

    pi_char = ['']

    for i in range(n):
        for j in range(n):

            if i == 0 and j == 0 or i == n-1 and j == n-1:

                continue

            pi_char.append(int_to_char[pi[i,j]])

    pi_char.append('')

    return np.asarray(pi_char).reshape(n,n)

"""# 1- Policy evaluation"""

def policy_evaluation(n, pi, v, Gamma, threshhold):

    while True:
        delta = 0.0
        v_new = v.copy()

        for i in range(n):
            for j in range(n):

                
                if is_terminal(i, j, n):
                    continue

                a = pi[i, j]              
                ni, nj = step(i, j, a, n)  

                # récompense = -1 
                target = -1.0 + Gamma * v[ni, nj]

                delta = max(delta, abs(target - v[i, j]))
                v_new[i, j] = target

        v = v_new

        if delta <= threshhold:
            break

    return v


"""# 2- Policy improvement"""

def policy_improvement(n, pi, v, Gamma):


    policy_stable = True
    new_pi = pi.copy()

    for i in range(n):
        for j in range(n):

            if is_terminal(i, j, n):
                continue

            old_action = pi[i, j]

            q_values = []
            for a in range(4):
                ni, nj = step(i, j, a, n)
                q = -1.0 + Gamma * v[ni, nj]
                q_values.append(q)

            best_action = int(np.argmax(q_values))
            new_pi[i, j] = best_action

            if best_action != old_action:
                policy_stable = False

    return new_pi, policy_stable

"""# 3- Policy Initialization"""

def policy_initialization(n):


    pi = np.random.choice(4, size=(n, n))

    # juste pour être propre : mettre quelque chose aux terminaux
    pi[0, 0] = 0
    pi[n-1, n-1] = 0

    return pi


"""# 4- Policy Iteration algorithm"""

def policy_iteration(n,Gamma,threshhold):

    pi = policy_initialization(n=n)

    v = np.zeros(shape=(n,n))

    while True:

        v = policy_evaluation(n=n,v=v,pi=pi,threshhold=threshhold,Gamma=Gamma)

        pi , pi_stable = policy_improvement(n=n,pi=pi,v=v,Gamma=Gamma)

        if pi_stable:

            break

    return pi , v

def value_iteration(n, Gamma, threshhold):
 

    v = np.zeros((n, n))

    while True:
        delta = 0.0
        v_new = v.copy()

        for i in range(n):
            for j in range(n):

                if is_terminal(i, j, n):
                    continue

                q_values = []
                for a in range(4):
                    ni, nj = step(i, j, a, n)
                    q_values.append(-1.0 + Gamma * v[ni, nj])

                best_value = np.max(q_values)
                delta = max(delta, abs(best_value - v[i, j]))
                v_new[i, j] = best_value

        v = v_new

        if delta <= threshhold:
            break

    
    pi = np.zeros((n, n), dtype=int)

    for i in range(n):
        for j in range(n):

            if is_terminal(i, j, n):
                continue

            q_values = []
            for a in range(4):
                ni, nj = step(i, j, a, n)
                q_values.append(-1.0 + Gamma * v[ni, nj])

            pi[i, j] = int(np.argmax(q_values))

    return pi, v


def is_terminal(i, j, n):
  """Les états terminaux sont les cases (0,0) et (n-1,n-1)."""

  return (i == 0 and j == 0) or (i == n-1 and j == n-1)

def step(i, j, a, n):
    """
    Transition déterministe :
    - on applique le déplacement policy_one_step_look_ahead[a]
    - si on sort de la grille, on reste sur place
    """

    di, dj = policy_one_step_look_ahead[a]
    ni, nj = i + di, j + dj

    if ni < 0 or ni >= n or nj < 0 or nj >= n:
        
        return i, j

    return ni, nj


"""# Main Code to Test"""

n = 4

Gamma = [0.8,0.9,1]

threshhold = 1e-4

for _gamma in Gamma:

    pi , v = policy_iteration(n=n,Gamma=_gamma,threshhold=threshhold)

    pi_char = policy_int_to_char(n=n,pi=pi)

    print()
    print("Gamma = ",_gamma)

    print()

    print(pi_char)

    print()
    print()

    print(v)

    pi_vi, v_vi = value_iteration(n=n, Gamma=_gamma, threshhold=threshhold)
    print(policy_int_to_char(pi=pi_vi, n=n))
    print(v_vi)


